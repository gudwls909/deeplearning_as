{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br>Assignment #5 Part 1: Implementing and Training a Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Hyemi Jang, November 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will implement one of famous reinforcement learning algorithm, Deep Q-Network (DQN) of DeepMind. <br>\n",
    "The goal here is to understand a basic form of DQN [1, 2] and learn how to use OpenAI Gym toolkit [3].<br>\n",
    "You need to follow the instructions to implement the given classes.\n",
    "\n",
    "1. [Play](#play) ( 50 points )\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **two parts of the assignment**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; Team_#)\n",
    "\n",
    "### Some helpful references for assignment #4 :\n",
    "- [1] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013). [[pdf]](https://www.google.co.kr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiI3aqPjavVAhXBkJQKHZsIDpgQFgg7MAI&url=https%3A%2F%2Fwww.cs.toronto.edu%2F~vmnih%2Fdocs%2Fdqn.pdf&usg=AFQjCNEd1AJoM72DeDpI_GBoPuv7NnVoFA)\n",
    "- [2] Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529-533. [[pdf]](https://www.nature.com/nature/journal/v518/n7540/pdf/nature14236.pdf)\n",
    "- [3] OpenAI GYM website [[link]](https://gym.openai.com/envs) and [[git]](https://github.com/openai/gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. OpenAI Gym\n",
    "\n",
    "OpenAI Gym is a toolkit to support diverse environments for developing reinforcement learning algorithms. You can use the toolkit with Python as well as TensorFlow. Installation guide of OpenAI Gym is offered by [this link](https://github.com/openai/gym#installation) or just type the command \"pip install gym\" (as well as \"pip install gym[atari]\" for Part2). \n",
    "\n",
    "After you set up OpenAI Gym, you can use APIs of the toolkit by inserting <font color=red>import gym</font> into your code. In this assignment, you must build one of famous reinforcement learning algorithms whose agent can run on OpenAI Gym environments. Please check how to use APIs such as funcions interacting with environments in the followings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2 \n",
    "import gym\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# Make an environment instance of CartPole-v0.\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Before interacting with the environment and starting a new episode, you must reset the environment's state.\n",
    "state = env.reset()\n",
    "\n",
    "# Uncomment to show the screenshot of the environment (rendering game screens)\n",
    "# env.render() \n",
    "\n",
    "# You can check action space and state (observation) space.\n",
    "num_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "print(num_actions)\n",
    "print(state_shape)\n",
    "\n",
    "# \"step\" function performs agent's actions given current state of the environment and returns several values.\n",
    "# Input: action (numerical data)\n",
    "#        - env.action_space.sample(): select a random action among possible actions.\n",
    "# Output: next_state (numerical data, next state of the environment after performing given action)\n",
    "#         reward (numerical data, reward of given action given current state)\n",
    "#         terminal (boolean data, True means the agent is done in the environment)\n",
    "next_state, reward, terminal, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a DQN agent\n",
    "## 1) Overview of implementation in the notebook\n",
    "\n",
    "The assignment is based on a method named by Deep Q-Network (DQN) [1,2]. You could find the details of DQN in the papers. The followings show briefly architecture of DQN and its training computation flow.\n",
    "\n",
    "- (Pink flow) Play an episode and save transition records of the episode into a replay memory.\n",
    "- (Green flow) Train DQN so that a loss function in the figure is minimized. The loss function is computed using main Q-network and Target Q-network. Target Q-network needs to be periodically updated by copying the main Q-network.\n",
    "- (Purple flow) Gradient can be autonomously computed by tensorflow engine, if you build a proper optimizer.\n",
    "\n",
    "![](image/architecture.png)\n",
    "\n",
    "There are major 4 components, each of which needs to be implemented in this notebook. The Agent class must have an instance(s) of each class (Environment, DQN, ReplayMemory).\n",
    "- Environment\n",
    "- DQN \n",
    "- ReplayMemory\n",
    "- Agent\n",
    "\n",
    "![](image/components.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Design classes\n",
    "\n",
    "In the code cells, there are only names of functions which are used in TA's implementation and their brief explanations. <font color='green'>...</font> means that the functions need more arguments and <font color='green'>pass</font> means that you need to write more codes. The functions may be helpful when you do not know how to start the assignment. Of course, you could change the functions such as deleting/adding functions or extending/reducing roles of the classes, <font color='red'> just keeping the existence of the classes</font>.\n",
    "\n",
    "### Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Environment(object):\n",
    "    def __init__(self, env, state_size, action_size):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        pass\n",
    "    \n",
    "    def random_action(self):\n",
    "        # Return a random action.\n",
    "        return random.randrange(self.action_size)\n",
    "        pass\n",
    "    \n",
    "    def render_worker(self, render=False):\n",
    "        # If display in your option is true, do rendering. Otherwise, do not.\n",
    "        if render:\n",
    "            self.env.render()\n",
    "        pass\n",
    "    \n",
    "    def new_episode(self):\n",
    "        # Start a new episode and return the first state of the new episode.\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        return state\n",
    "        pass\n",
    "    \n",
    "    def act(self, action):\n",
    "        # Perform an action which is given by input argument and return the results of acting.\n",
    "        next_state, reward, terminal, _ = self.env.step(action)\n",
    "        return next_state, reward, terminal\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayMemory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, state_size, batch_size):\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        pass\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, terminal):\n",
    "        # Add current_state, action, reward, terminal, (next_state which can be added by your choice). \n",
    "        self.memory.append((state, action, reward, next_state, terminal))\n",
    "        pass\n",
    "    \n",
    "    def mini_batch(self):\n",
    "        # Return a mini_batch whose data are selected according to your sampling method. (such as uniform-random sampling in DQN papers)\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, terminals = [], [], []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            terminals.append(mini_batch[i][4])\n",
    "            \n",
    "        return states, actions, rewards, next_states, terminals            \n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import Dense\n",
    "#from keras.optimizers import Adam\n",
    "#from keras.models import Sequential\n",
    "class DQN(object):\n",
    "    def __init__(self, state_size, action_size, learning_rate, replay, batch_size, discount_factor):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.replay = replay\n",
    "        self.batch_size = batch_size\n",
    "        self.discount_facter = discount_factor\n",
    "        self.prediction_Q = self.build_network('pred')\n",
    "        self.target_Q = self.build_network('target')\n",
    "        pass\n",
    "    \n",
    "    def build_network(self, name):\n",
    "        # Make your a deep neural network\n",
    "        with tf.variable_scope(name , reuse=tf.AUTO_REUSE):\n",
    "            model = tf.keras.Sequential()\n",
    "            model.add(tf.layers.Dense(25, input_dim=self.state_size, activation='relu', \n",
    "                            kernel_initializer='he_uniform'))\n",
    "            model.add(tf.layers.Dense(25, activation='relu', kernel_initializer='he_uniform'))\n",
    "            model.add(tf.layers.Dense(25, activation='relu', kernel_initializer='he_uniform'))\n",
    "            model.add(tf.layers.Dense(self.action_size, activation='relu', \n",
    "                            kernel_initializer='he_uniform'))\n",
    "            model.summary()\n",
    "            model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "            return model\n",
    "            pass\n",
    "                \n",
    "        #copy_op = []\n",
    "        #pred_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='pred')\n",
    "        #target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='target')\n",
    "        #for pred_var, target_var in zip(pred_vars, target_vars):\n",
    "        #    copy_op.append(target_var.assign(pred_var.value()))\n",
    "    \n",
    "    #def build_optimizer(self):\n",
    "        # Make your optimizer \n",
    "    #    pass\n",
    "    \n",
    "    def train_network(self, discount_factor):\n",
    "        # Train the prediction_Q network using a mini-batch sampled from the replay memory\n",
    "        states, actions, rewards, next_states, terminals = self.replay.mini_batch()\n",
    "        \n",
    "        pred_Q = self.prediction_Q.predict(states)\n",
    "        tar_Q = self.target_Q.predict(next_states)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if terminals[i]:\n",
    "                pred_Q[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                pred_Q[i][actions[i]] = rewards[i] + discount_factor*(np.amax(tar_Q[i]))\n",
    "                \n",
    "        self.prediction_Q.fit(states, pred_Q, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "        pass\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        #self.sess.run(copy_op)\n",
    "        self.target_Q.set_weights(self.prediction_Q.get_weights())\n",
    "    \n",
    "    #def predict_Q(self, ...):\n",
    "    #    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # to save and load\n",
    "import random\n",
    "class Agent(object):\n",
    "    def __init__(self, args, mode):\n",
    "        self.env = gym.make(args.env_name)\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        #self.saver = tf.train.Saver()\n",
    "        if mode=='train':\n",
    "            self.epsilon = 1.0\n",
    "        elif mode=='test':\n",
    "            self.epsilon = 0.0\n",
    "        else:\n",
    "            raise Exception(\"mode type not supported: {}\".format(mode))\n",
    "        self.epsilon_decay_steps = args.epsilon_decay_steps\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.batch_size = args.batch_size\n",
    "        self.discount_factor = args.discount_factor\n",
    "        self.episodes = args.episodes\n",
    "        self.ENV = Environment(self.env, self.state_size, self.action_size)\n",
    "        self.replay = ReplayMemory(self.state_size, self.batch_size)\n",
    "        self.dqn = DQN(self.state_size, self.action_size, self.learning_rate, \n",
    "                       self.replay, self.batch_size, self.discount_factor)\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # Select an action according Îµ-greedy. You need to use a random-number generating function and add a library if necessary.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.dqn.prediction_Q.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        # Train your agent which has the neural nets.\n",
    "        # Several hyper-parameters are determined by your choice (Options class in the below cell)\n",
    "        # Keep epsilon-greedy action selection in your mind \n",
    "                \n",
    "        scores, episodes = [], []\n",
    "        \n",
    "        for e in range(self.episodes):\n",
    "            terminal = False\n",
    "            score = 0\n",
    "            state = self.ENV.new_episode()\n",
    "            \n",
    "            int_e = 0\n",
    "            while not terminal:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminal = self.ENV.act(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                self.replay.add(state, action, reward, next_state, terminal)\n",
    "                \n",
    "                if len(self.replay.memory)>=1000:\n",
    "                    if self.epsilon > 0.1:\n",
    "                        self.epsilon -= 0.9/1e4\n",
    "                    self.dqn.train_network(self.discount_factor)\n",
    "                    \n",
    "                score += reward\n",
    "                state = next_state\n",
    "                int_e += 1\n",
    "                \n",
    "                if terminal:\n",
    "                    self.dqn.update_target_network()\n",
    "                    scores.append(score)\n",
    "                    episodes.append(e)\n",
    "                    print('episode:', e, ' score:', score, ' epsilon', self.epsilon, \n",
    "                          ' last 10 mean score', np.mean(scores[-min(10, len(scores)):]))\n",
    "                    \n",
    "                    if np.mean(scores[-min(10, len(scores)):]) > 195:\n",
    "                        print('Already well trained')\n",
    "                        return\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    def play(self, test=False):\n",
    "        # Test your agent \n",
    "        # When performing test, you can show the environment's screen by rendering,\n",
    "        state = self.ENV.new_episode()\n",
    "        self.ENV.render_worker(test)\n",
    "        \n",
    "        terminal = False\n",
    "        score = 0\n",
    "        while not terminal:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminal = self.ENV.act(action)\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminal:\n",
    "                return score\n",
    "        pass\n",
    "    \n",
    "    def save(self):\n",
    "        #checkpoint_dir = 'cartpole'\n",
    "        #if not os.path.exists(checkpoint_dir):\n",
    "        #    os.mkdir(checkpoint_dir)\n",
    "        #self.saver.save(self.sess, os.path.join(checkpoint_dir, 'trained_agent'))\n",
    "        self.dqn.prediction_Q.save_weights(\"./save_model/dqn.h5\")\n",
    "        \n",
    "    def load(self):\n",
    "        #checkpoint_dir = 'cartpole'\n",
    "        #self.saver.restore(self.sess, os.path.join(checkpoint_dir, 'trained_agent'))\n",
    "        self.dqn.prediction_Q.load_weights(\"./save_model/dqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train your agent \n",
    "\n",
    "Now, you train an agent to play CartPole-v0. Options class is the collection of hyper-parameters that you can choice. Usage of Options class is not mandatory.<br>\n",
    "The maximum value of total reward which can be aquired from one episode is 200. \n",
    "<font color='red'>**You should show learning status such as the number of observed states and mean/max/min of rewards frequently (for instance, every 100 states).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, discount_factor=0.99, env_name='CartPole-v0', episodes=300, epsilon_decay_steps=1000, learning_rate=0.001)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "parser = argparse.ArgumentParser(description=\"CartPole\")\n",
    "parser.add_argument('--env_name', default='CartPole-v0', type=str,\n",
    "                    help=\"Environment\")\n",
    "#parser.add_argument('--render', default=False, type=bool)\n",
    "parser.add_argument('--epsilon_decay_steps', default=1000, type=int,\n",
    "                    help=\"how many steps for epsilon to be 0.1\")\n",
    "parser.add_argument('--learning_rate', default=0.001, type=float)\n",
    "parser.add_argument('--batch_size', default=64, type=int)\n",
    "parser.add_argument('--discount_factor', default=0.99, type=float)\n",
    "parser.add_argument('--episodes', default=300, type=float)\n",
    "sys.argv = ['-f']\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "config = tf.ConfigProto()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config.log_device_placement = False\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "#with tf.Session(config=config) as sess:\n",
    "#myAgent = Agent(args, 'train') # It depends on your class implementation\n",
    "#myAgent.train()\n",
    "#myAgent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                125       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 25)                125       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0  score: 19.0  epsilon 1.0  last 10 mean score 19.0\n",
      "episode: 1  score: 22.0  epsilon 1.0  last 10 mean score 20.5\n",
      "episode: 2  score: 14.0  epsilon 1.0  last 10 mean score 18.333333333333332\n",
      "episode: 3  score: 30.0  epsilon 1.0  last 10 mean score 21.25\n",
      "episode: 4  score: 20.0  epsilon 1.0  last 10 mean score 21.0\n",
      "episode: 5  score: 23.0  epsilon 1.0  last 10 mean score 21.333333333333332\n",
      "episode: 6  score: 12.0  epsilon 1.0  last 10 mean score 20.0\n",
      "episode: 7  score: 12.0  epsilon 1.0  last 10 mean score 19.0\n",
      "episode: 8  score: 18.0  epsilon 1.0  last 10 mean score 18.88888888888889\n",
      "episode: 9  score: 22.0  epsilon 1.0  last 10 mean score 19.2\n",
      "episode: 10  score: 14.0  epsilon 1.0  last 10 mean score 18.7\n",
      "episode: 11  score: 18.0  epsilon 1.0  last 10 mean score 18.3\n",
      "episode: 12  score: 21.0  epsilon 1.0  last 10 mean score 19.0\n",
      "episode: 13  score: 36.0  epsilon 1.0  last 10 mean score 19.6\n",
      "episode: 14  score: 34.0  epsilon 1.0  last 10 mean score 21.0\n",
      "episode: 15  score: 16.0  epsilon 1.0  last 10 mean score 20.3\n",
      "episode: 16  score: 12.0  epsilon 1.0  last 10 mean score 20.3\n",
      "episode: 17  score: 43.0  epsilon 1.0  last 10 mean score 23.4\n",
      "episode: 18  score: 26.0  epsilon 1.0  last 10 mean score 24.2\n",
      "episode: 19  score: 49.0  epsilon 1.0  last 10 mean score 26.9\n",
      "episode: 20  score: 13.0  epsilon 1.0  last 10 mean score 26.8\n",
      "episode: 21  score: 27.0  epsilon 1.0  last 10 mean score 27.7\n",
      "episode: 22  score: 23.0  epsilon 1.0  last 10 mean score 27.9\n",
      "episode: 23  score: 15.0  epsilon 1.0  last 10 mean score 25.8\n",
      "episode: 24  score: 39.0  epsilon 1.0  last 10 mean score 26.3\n",
      "episode: 25  score: 17.0  epsilon 1.0  last 10 mean score 26.4\n",
      "episode: 26  score: 13.0  epsilon 1.0  last 10 mean score 26.5\n",
      "episode: 27  score: 14.0  epsilon 1.0  last 10 mean score 23.6\n",
      "episode: 28  score: 32.0  epsilon 1.0  last 10 mean score 24.2\n",
      "episode: 29  score: 14.0  epsilon 1.0  last 10 mean score 20.7\n",
      "episode: 30  score: 16.0  epsilon 1.0  last 10 mean score 21.0\n",
      "episode: 31  score: 14.0  epsilon 1.0  last 10 mean score 19.7\n",
      "episode: 32  score: 11.0  epsilon 1.0  last 10 mean score 18.5\n",
      "episode: 33  score: 15.0  epsilon 1.0  last 10 mean score 18.5\n",
      "episode: 34  score: 38.0  epsilon 1.0  last 10 mean score 18.4\n",
      "episode: 35  score: 13.0  epsilon 1.0  last 10 mean score 18.0\n",
      "episode: 36  score: 35.0  epsilon 1.0  last 10 mean score 20.2\n",
      "episode: 37  score: 20.0  epsilon 1.0  last 10 mean score 20.8\n",
      "episode: 38  score: 10.0  epsilon 1.0  last 10 mean score 18.6\n",
      "episode: 39  score: 12.0  epsilon 1.0  last 10 mean score 18.4\n",
      "episode: 40  score: 42.0  epsilon 1.0  last 10 mean score 21.0\n",
      "episode: 41  score: 33.0  epsilon 1.0  last 10 mean score 22.9\n",
      "episode: 42  score: 13.0  epsilon 1.0  last 10 mean score 23.1\n",
      "episode: 43  score: 18.0  epsilon 1.0  last 10 mean score 23.4\n",
      "episode: 44  score: 41.0  epsilon 1.0  last 10 mean score 23.7\n",
      "episode: 45  score: 11.0  epsilon 0.9990099999999996  last 10 mean score 23.5\n",
      "episode: 46  score: 15.0  epsilon 0.9976599999999991  last 10 mean score 21.5\n",
      "episode: 47  score: 14.0  epsilon 0.9963999999999986  last 10 mean score 20.9\n",
      "episode: 48  score: 14.0  epsilon 0.9951399999999981  last 10 mean score 21.3\n",
      "episode: 49  score: 12.0  epsilon 0.9940599999999977  last 10 mean score 21.3\n",
      "episode: 50  score: 23.0  epsilon 0.9919899999999969  last 10 mean score 19.4\n",
      "episode: 51  score: 18.0  epsilon 0.9903699999999963  last 10 mean score 17.9\n",
      "episode: 52  score: 11.0  epsilon 0.9893799999999959  last 10 mean score 17.7\n",
      "episode: 53  score: 18.0  epsilon 0.9877599999999953  last 10 mean score 17.7\n",
      "episode: 54  score: 21.0  epsilon 0.9858699999999946  last 10 mean score 15.7\n",
      "episode: 55  score: 15.0  epsilon 0.9845199999999941  last 10 mean score 16.1\n",
      "episode: 56  score: 20.0  epsilon 0.9827199999999934  last 10 mean score 16.6\n",
      "episode: 57  score: 19.0  epsilon 0.9810099999999927  last 10 mean score 17.1\n",
      "episode: 58  score: 10.0  epsilon 0.9801099999999924  last 10 mean score 16.7\n",
      "episode: 59  score: 26.0  epsilon 0.9777699999999915  last 10 mean score 18.1\n",
      "episode: 60  score: 61.0  epsilon 0.9722799999999894  last 10 mean score 21.9\n",
      "episode: 61  score: 18.0  epsilon 0.9706599999999888  last 10 mean score 21.9\n",
      "episode: 62  score: 11.0  epsilon 0.9696699999999884  last 10 mean score 21.9\n",
      "episode: 63  score: 11.0  epsilon 0.968679999999988  last 10 mean score 21.2\n",
      "episode: 64  score: 41.0  epsilon 0.9649899999999866  last 10 mean score 23.2\n",
      "episode: 65  score: 18.0  epsilon 0.963369999999986  last 10 mean score 23.5\n",
      "episode: 66  score: 39.0  epsilon 0.9598599999999846  last 10 mean score 25.4\n",
      "episode: 67  score: 23.0  epsilon 0.9577899999999838  last 10 mean score 25.8\n",
      "episode: 68  score: 15.0  epsilon 0.9564399999999833  last 10 mean score 26.3\n",
      "episode: 69  score: 10.0  epsilon 0.955539999999983  last 10 mean score 24.7\n",
      "episode: 70  score: 19.0  epsilon 0.9538299999999823  last 10 mean score 20.5\n",
      "episode: 71  score: 15.0  epsilon 0.9524799999999818  last 10 mean score 20.2\n",
      "episode: 72  score: 25.0  epsilon 0.9502299999999809  last 10 mean score 21.6\n",
      "episode: 73  score: 14.0  epsilon 0.9489699999999804  last 10 mean score 21.9\n",
      "episode: 74  score: 58.0  epsilon 0.9437499999999784  last 10 mean score 23.6\n",
      "episode: 75  score: 46.0  epsilon 0.9396099999999769  last 10 mean score 26.4\n",
      "episode: 76  score: 25.0  epsilon 0.937359999999976  last 10 mean score 25.0\n",
      "episode: 77  score: 26.0  epsilon 0.9350199999999751  last 10 mean score 25.3\n",
      "episode: 78  score: 14.0  epsilon 0.9337599999999746  last 10 mean score 25.2\n",
      "episode: 79  score: 14.0  epsilon 0.9324999999999741  last 10 mean score 25.6\n",
      "episode: 80  score: 95.0  epsilon 0.9239499999999709  last 10 mean score 33.2\n",
      "episode: 81  score: 15.0  epsilon 0.9225999999999703  last 10 mean score 33.2\n",
      "episode: 82  score: 14.0  epsilon 0.9213399999999698  last 10 mean score 32.1\n",
      "episode: 83  score: 44.0  epsilon 0.9173799999999683  last 10 mean score 35.1\n",
      "episode: 84  score: 31.0  epsilon 0.9145899999999673  last 10 mean score 32.4\n",
      "episode: 85  score: 35.0  epsilon 0.911439999999966  last 10 mean score 31.3\n",
      "episode: 86  score: 44.0  epsilon 0.9074799999999645  last 10 mean score 33.2\n",
      "episode: 87  score: 16.0  epsilon 0.906039999999964  last 10 mean score 32.2\n",
      "episode: 88  score: 55.0  epsilon 0.9010899999999621  last 10 mean score 36.3\n",
      "episode: 89  score: 41.0  epsilon 0.8973999999999607  last 10 mean score 39.0\n",
      "episode: 90  score: 15.0  epsilon 0.8960499999999602  last 10 mean score 31.0\n",
      "episode: 91  score: 16.0  epsilon 0.8946099999999596  last 10 mean score 31.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 92  score: 39.0  epsilon 0.8910999999999583  last 10 mean score 33.6\n",
      "episode: 93  score: 37.0  epsilon 0.887769999999957  last 10 mean score 32.9\n",
      "episode: 94  score: 11.0  epsilon 0.8867799999999566  last 10 mean score 30.9\n",
      "episode: 95  score: 32.0  epsilon 0.8838999999999555  last 10 mean score 30.6\n",
      "episode: 96  score: 27.0  epsilon 0.8814699999999546  last 10 mean score 28.9\n",
      "episode: 97  score: 62.0  epsilon 0.8758899999999524  last 10 mean score 33.5\n",
      "episode: 98  score: 19.0  epsilon 0.8741799999999518  last 10 mean score 29.9\n",
      "episode: 99  score: 54.0  epsilon 0.8693199999999499  last 10 mean score 31.2\n",
      "episode: 100  score: 23.0  epsilon 0.8672499999999491  last 10 mean score 32.0\n",
      "episode: 101  score: 51.0  epsilon 0.8626599999999474  last 10 mean score 35.5\n",
      "episode: 102  score: 25.0  epsilon 0.8604099999999465  last 10 mean score 34.1\n",
      "episode: 103  score: 41.0  epsilon 0.8567199999999451  last 10 mean score 34.5\n",
      "episode: 104  score: 39.0  epsilon 0.8532099999999437  last 10 mean score 37.3\n",
      "episode: 105  score: 74.0  epsilon 0.8465499999999412  last 10 mean score 41.5\n",
      "episode: 106  score: 60.0  epsilon 0.8411499999999391  last 10 mean score 44.8\n",
      "episode: 107  score: 13.0  epsilon 0.8399799999999387  last 10 mean score 39.9\n",
      "episode: 108  score: 27.0  epsilon 0.8375499999999377  last 10 mean score 40.7\n",
      "episode: 109  score: 15.0  epsilon 0.8361999999999372  last 10 mean score 36.8\n",
      "episode: 110  score: 13.0  epsilon 0.8350299999999368  last 10 mean score 35.8\n",
      "episode: 111  score: 16.0  epsilon 0.8335899999999362  last 10 mean score 32.3\n",
      "episode: 112  score: 24.0  epsilon 0.8314299999999354  last 10 mean score 32.2\n",
      "episode: 113  score: 49.0  epsilon 0.8270199999999337  last 10 mean score 33.0\n",
      "episode: 114  score: 74.0  epsilon 0.8203599999999311  last 10 mean score 36.5\n",
      "episode: 115  score: 45.0  epsilon 0.8163099999999296  last 10 mean score 33.6\n",
      "episode: 116  score: 32.0  epsilon 0.8134299999999285  last 10 mean score 30.8\n",
      "episode: 117  score: 25.0  epsilon 0.8111799999999276  last 10 mean score 32.0\n",
      "episode: 118  score: 37.0  epsilon 0.8078499999999263  last 10 mean score 33.0\n",
      "episode: 119  score: 15.0  epsilon 0.8064999999999258  last 10 mean score 33.0\n",
      "episode: 120  score: 12.0  epsilon 0.8054199999999254  last 10 mean score 32.9\n",
      "episode: 121  score: 22.0  epsilon 0.8034399999999247  last 10 mean score 33.5\n",
      "episode: 122  score: 22.0  epsilon 0.8014599999999239  last 10 mean score 33.3\n",
      "episode: 123  score: 73.0  epsilon 0.7948899999999214  last 10 mean score 35.7\n",
      "episode: 124  score: 49.0  epsilon 0.7904799999999197  last 10 mean score 33.2\n",
      "episode: 125  score: 12.0  epsilon 0.7893999999999193  last 10 mean score 29.9\n",
      "episode: 126  score: 30.0  epsilon 0.7866999999999182  last 10 mean score 29.7\n",
      "episode: 127  score: 81.0  epsilon 0.7794099999999154  last 10 mean score 35.3\n",
      "episode: 128  score: 11.0  epsilon 0.7784199999999151  last 10 mean score 32.7\n",
      "episode: 129  score: 27.0  epsilon 0.7759899999999141  last 10 mean score 33.9\n",
      "episode: 130  score: 40.0  epsilon 0.7723899999999128  last 10 mean score 36.7\n",
      "episode: 131  score: 36.0  epsilon 0.7691499999999115  last 10 mean score 38.1\n",
      "episode: 132  score: 36.0  epsilon 0.7659099999999103  last 10 mean score 39.5\n",
      "episode: 133  score: 18.0  epsilon 0.7642899999999097  last 10 mean score 34.0\n",
      "episode: 134  score: 27.0  epsilon 0.7618599999999087  last 10 mean score 31.8\n",
      "episode: 135  score: 25.0  epsilon 0.7596099999999079  last 10 mean score 33.1\n",
      "episode: 136  score: 87.0  epsilon 0.7517799999999049  last 10 mean score 38.8\n",
      "episode: 137  score: 68.0  epsilon 0.7456599999999025  last 10 mean score 37.5\n",
      "episode: 138  score: 25.0  epsilon 0.7434099999999016  last 10 mean score 38.9\n",
      "episode: 139  score: 18.0  epsilon 0.741789999999901  last 10 mean score 38.0\n",
      "episode: 140  score: 27.0  epsilon 0.7393599999999001  last 10 mean score 36.7\n",
      "episode: 141  score: 20.0  epsilon 0.7375599999998994  last 10 mean score 35.1\n",
      "episode: 142  score: 70.0  epsilon 0.731259999999897  last 10 mean score 38.5\n",
      "episode: 143  score: 47.0  epsilon 0.7270299999998954  last 10 mean score 41.4\n",
      "episode: 144  score: 33.0  epsilon 0.7240599999998942  last 10 mean score 42.0\n",
      "episode: 145  score: 78.0  epsilon 0.7170399999998915  last 10 mean score 47.3\n",
      "episode: 146  score: 42.0  epsilon 0.7132599999998901  last 10 mean score 42.8\n",
      "episode: 147  score: 14.0  epsilon 0.7119999999998896  last 10 mean score 37.4\n",
      "episode: 148  score: 42.0  epsilon 0.7082199999998882  last 10 mean score 39.1\n",
      "episode: 149  score: 61.0  epsilon 0.7027299999998861  last 10 mean score 43.4\n",
      "episode: 150  score: 83.0  epsilon 0.6952599999998832  last 10 mean score 49.0\n",
      "episode: 151  score: 25.0  epsilon 0.6930099999998823  last 10 mean score 49.5\n",
      "episode: 152  score: 43.0  epsilon 0.6891399999998808  last 10 mean score 46.8\n",
      "episode: 153  score: 26.0  epsilon 0.68679999999988  last 10 mean score 44.7\n",
      "episode: 154  score: 92.0  epsilon 0.6785199999998768  last 10 mean score 50.6\n",
      "episode: 155  score: 30.0  epsilon 0.6758199999998757  last 10 mean score 45.8\n",
      "episode: 156  score: 57.0  epsilon 0.6706899999998738  last 10 mean score 47.3\n",
      "episode: 157  score: 22.0  epsilon 0.668709999999873  last 10 mean score 48.1\n",
      "episode: 158  score: 29.0  epsilon 0.666099999999872  last 10 mean score 46.8\n",
      "episode: 159  score: 26.0  epsilon 0.6637599999998711  last 10 mean score 43.3\n",
      "episode: 160  score: 121.0  epsilon 0.652869999999867  last 10 mean score 47.1\n",
      "episode: 161  score: 16.0  epsilon 0.6514299999998664  last 10 mean score 46.2\n",
      "episode: 162  score: 92.0  epsilon 0.6431499999998632  last 10 mean score 51.1\n",
      "episode: 163  score: 120.0  epsilon 0.6323499999998591  last 10 mean score 60.5\n",
      "episode: 164  score: 15.0  epsilon 0.6309999999998586  last 10 mean score 52.8\n",
      "episode: 165  score: 65.0  epsilon 0.6251499999998563  last 10 mean score 56.3\n",
      "episode: 166  score: 184.0  epsilon 0.60858999999985  last 10 mean score 69.0\n",
      "episode: 167  score: 13.0  epsilon 0.6074199999998495  last 10 mean score 68.1\n",
      "episode: 168  score: 94.0  epsilon 0.5989599999998463  last 10 mean score 74.6\n",
      "episode: 169  score: 89.0  epsilon 0.5909499999998432  last 10 mean score 80.9\n",
      "episode: 170  score: 17.0  epsilon 0.5894199999998426  last 10 mean score 70.5\n",
      "episode: 171  score: 79.0  epsilon 0.5823099999998399  last 10 mean score 76.8\n",
      "episode: 172  score: 60.0  epsilon 0.5769099999998378  last 10 mean score 73.6\n",
      "episode: 173  score: 75.0  epsilon 0.5701599999998352  last 10 mean score 69.1\n",
      "episode: 174  score: 171.0  epsilon 0.5547699999998293  last 10 mean score 84.7\n",
      "episode: 175  score: 24.0  epsilon 0.5526099999998285  last 10 mean score 80.6\n",
      "episode: 176  score: 200.0  epsilon 0.5346099999998216  last 10 mean score 82.2\n",
      "episode: 177  score: 192.0  epsilon 0.517329999999815  last 10 mean score 100.1\n",
      "episode: 178  score: 71.0  epsilon 0.5109399999998125  last 10 mean score 97.8\n",
      "episode: 179  score: 149.0  epsilon 0.49752999999980896  last 10 mean score 103.8\n",
      "episode: 180  score: 180.0  epsilon 0.48132999999981274  last 10 mean score 120.1\n",
      "episode: 181  score: 116.0  epsilon 0.4708899999998152  last 10 mean score 123.8\n",
      "episode: 182  score: 36.0  epsilon 0.46764999999981594  last 10 mean score 121.4\n",
      "episode: 183  score: 200.0  epsilon 0.44964999999982014  last 10 mean score 133.9\n",
      "episode: 184  score: 75.0  epsilon 0.4428999999998217  last 10 mean score 124.3\n",
      "episode: 185  score: 61.0  epsilon 0.437409999999823  last 10 mean score 128.0\n",
      "episode: 186  score: 190.0  epsilon 0.420309999999827  last 10 mean score 127.0\n",
      "episode: 187  score: 158.0  epsilon 0.4060899999998303  last 10 mean score 123.6\n",
      "episode: 188  score: 200.0  epsilon 0.3880899999998345  last 10 mean score 136.5\n",
      "episode: 189  score: 147.0  epsilon 0.3748599999998376  last 10 mean score 136.3\n",
      "episode: 190  score: 12.0  epsilon 0.37377999999983785  last 10 mean score 119.5\n",
      "episode: 191  score: 15.0  epsilon 0.37242999999983817  last 10 mean score 109.4\n",
      "episode: 192  score: 200.0  epsilon 0.35442999999984237  last 10 mean score 125.8\n",
      "episode: 193  score: 200.0  epsilon 0.3364299999998466  last 10 mean score 125.8\n",
      "episode: 194  score: 171.0  epsilon 0.32103999999985017  last 10 mean score 135.4\n",
      "episode: 195  score: 200.0  epsilon 0.30303999999985437  last 10 mean score 149.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 196  score: 200.0  epsilon 0.2850399999998586  last 10 mean score 150.3\n",
      "episode: 197  score: 200.0  epsilon 0.2670399999998628  last 10 mean score 154.5\n",
      "episode: 198  score: 200.0  epsilon 0.24903999999986667  last 10 mean score 154.5\n",
      "episode: 199  score: 200.0  epsilon 0.23103999999986533  last 10 mean score 159.8\n",
      "episode: 200  score: 200.0  epsilon 0.21303999999986398  last 10 mean score 178.6\n",
      "episode: 201  score: 200.0  epsilon 0.19503999999986263  last 10 mean score 197.1\n",
      "Already well trained\n"
     ]
    }
   ],
   "source": [
    "#with tf.Session(config=config) as sess:\n",
    "myAgent = Agent(args, 'train') # It depends on your class implementation\n",
    "myAgent.train()\n",
    "myAgent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"play\"></a> 3. Test the trained agent ( 15 points )\n",
    "\n",
    "Now, we test your agent and calculate an average reward of 20 episodes.\n",
    "- 0 <= average reward < 50 : you can get 0 points\n",
    "- 50 <= average reward < 100 : you can get 10 points\n",
    "- 100 <= average reward < 190 : you can get 35 points\n",
    "- 190 <= average reward <= 200 : you can get 50 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 25)                125       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 25)                125       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "#config = tf.ConfigProto()\n",
    "# If you use a GPU, uncomment\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#config.log_device_placement = False\n",
    "# config.gpu_options.allow_growth = True\n",
    "#with tf.Session(config=config) as sess:\n",
    "#args = parser.parse_args() # You set the option of test phase\n",
    "myAgent = Agent(args, 'test') # It depends on your class implementation\n",
    "myAgent.load()\n",
    "rewards = []\n",
    "for i in range(20):\n",
    "    r = myAgent.play() # play() returns the reward cumulated in one episode\n",
    "    rewards.append(r)\n",
    "mean = np.mean(rewards)\n",
    "print(rewards)\n",
    "print(mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
